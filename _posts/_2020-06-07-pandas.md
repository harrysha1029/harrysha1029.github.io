---
title: "Working with Pandas"
tags: cs
---

Recently, I completed an internship that involved working with a lot of data writing pandas code pretty much every day. Here are a couple of takeaways from that experience!

## Loading and saving data
This function is super useful for reading in data but in its base form has some major flaws. Check this out:

<pre class="command-line language-bash" data-prompt=">>>" data-output="4-6,10-12"><code class="language-python">
import pandas as pd
df = pd.DataFrame(["01", "1"], columns=['col'])
df
  col
0  01
1  1
df.to_csv("test.csv", index=False)
loaded = pd.read_csv("test.csv")
loaded
  col
0  1
1  1
</code></pre>

Uh oh. When loading the csv file, `read_csv` automatically parsed the column to have type integer :(. This is just an example of how `to_csv` and `read_csv` do not exactly invert one another, and this can cause a lot of problems and headaches (I can no longer distinguish between those two values!).  

Now there are several ways to deal with this. The first is to not use the csv format. feather-format `to_feather, read_feather`, does not suffer from this problem as it stores type information. However, the resulting files are not human readable, which is slightly annoying (I ultimately needed to export data into csv). The second is to pass a `dtype` argument to `read_csv` to correctly load the DataFrame. 

Even this is annoying though because you have to remember and type out all the types for all the columns in your csv file. I ended up doing the following. The idea here is to **store a single constant dictionary containing the type information for all the columns in all the data in a project.** Then write some helper functions to load data as follows. 

<pre><code class="language-python">
MASTER_TYPES: PandasTypes = {
    "fips": str,
    "naics": str,
    "weight": float,
    "year": "int16",
    "month": "int8",
    "qtr": "int8",
    ...
}

def read_cols(path: PathLike):
    return pd.read_csv(path, nrows=0).columns.tolist()

def load(path)
    cols = read_cols(path)
    dtypes = {c: MASTER_TYPES[c] for c in cols}
    return pd.read_csv(path, dtype=dtypes)
</code></pre>

The constraint here is that if you have two datasets with the same column names, they must have the same type, but this seems somewhat reasonable. In practice, I actually have some more helper functions that save the data in both feather-format and csv because feather loads much quicker. I also have another wrapper to refer to datasets by name instead of by path.

## Style and readability
Pandas code can get hella confusing. You might start off by loading in a `DataFrame` then start performing merges, adding columns, call another function on the DataFrame. While this is happening, someone else reading the code is sure to get lost. I found this happening to myself countless of times when getting acquainted with the code base. I'd wonder *'What columns are in the DataFrame at this point?'* and have to either put in print statements in the code or try and execute the code step by step in a Jupyter Notebook.

To try an improve the readability and style of the code I tried two main things - **method chaining**, and **'type/column'** checking. 

### Method chaining
Pandas has a nice thing were most of its functions return `DataFrames`. Even functions that you might no expect to such as `rename` or `astype` will return the `DataFrame` rather than changing them in place. Here's an example of this style of code: TODO come up with an example.

<!-- <pre><code class="language-python">
    return (
        starting_df.rename(columns={"emp": "start"})
        .merge(county_df, how="left")
        .assign(state=lambda x: x.fips.map(lambda y: y[:2] + "000"))
        .groupby(["naics"])
        .progress_apply(_helper_naics, state_df=state_df, national_df=national_df)
        .reset_index()
        .assign(
            year=lambda x: x.time_step.map(lambda y: TIME_STEP_2_YEAR_MONTH[y][0]),
            month=lambda x: x.time_step.map(lambda y: TIME_STEP_2_YEAR_MONTH[y][1]),
        )[QCEW_EMP_COLS]
        .pipe(save_processed, name="naics_employment_2016_m3_to_2019_m9")
    )
</code></pre> -->

Of note here, are the functions [assign](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html), and [pipe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pipe.html), which make this style of code possible. 

Method chaining makes the code easier to read because it very clearly shows the transformations being done data in a linear fashion. This however can be a pain to debug, but completely possible with some helper functions. This function `pd_print` was really helpful in seeing how each operation transformed the data.

<pre><code class="language-python">
def pd_print(df, f=identity):
    print(f(df))
    return df
</code></pre>

Essentially it takes in a dataframe and optionally a function, and prints the result of the function applied on the dataframe and then returns the dataframe unedited. Because it essentially does nothing to the dataframe, we can litter our method chained calls with 

<pre><code class="language-python">
.pipe(pd_print)
</code></pre>

to print intermediate steps in our processing. We can use the f argument to just print the types if we want:

<pre><code class="language-python">
.pipe(pd_print, lambda x: x.dtypes)
</code></pre>

### Type and column checking
The idea here is to enforce the existence of certain columns and their types in the inputs and outputs of functions. To do this, I implemented decorator generator that works on functions with dataframes as arguments or outputs. It can be used like this

<pre><code class="language-python">
@pd_type_check([TYPES1, TYPES2], TYPES3)
def some_function_on_dfs(df1, df2):
    ...
</code></pre>

Then, whenever this function is called, the decorated function first checks that the columns and types of the arguments are exactly those specified by `TYPES1` and `TYPES2`, then runs the function, and checks that the output has the exact types and columns specified by `TYPES3`. If any of the checks fail, it will raise and error. This **removes any ambiguity as to which columns and types the DataFrame has before/after a function.**

Currently, I'm extending this to work with arguments that can have a variable number of columns, or have columns specified by another argument to the function and will release this eventually!