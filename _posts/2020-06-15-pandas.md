---
title: "Working with Pandas"
tags: cs
---

Recently, I completed an internship that involved working with a lot of data writing pandas code pretty much every day. Here are a couple of takeaways from that experience!

## Loading and saving data
This function is super useful for reading in data but in its base form has some major flaws. Check this out:

<pre class="command-line language-bash" data-prompt=">>>" data-output="4-6,10-12"><code class="language-python">
import pandas as pd
df = pd.DataFrame(["01", "1"], columns=['col'])
df
  col
0  01
1  1
df.to_csv("test.csv", index=False)
loaded = pd.read_csv("test.csv")
loaded
  col
0  1
1  1
</code></pre>

Uh oh. When loading the csv file, `read_csv` automatically parsed the column to have type integer :(. This is just an example of how `to_csv` and `read_csv` do not exactly invert one another, and this can cause a lot of problems and headaches (I can no longer distinguish between those two values!).  

Now there are several ways to deal with this. The first is to not use the csv format. feather-format `to_feather, read_feather`, does not suffer from this problem as it stores type information. However, the resulting files are not human readable, which is slightly annoying (I ultimately needed to export data into csv). The second is to pass a `dtype` argument to `read_csv` to correctly load the DataFrame. 

Even this is annoying though because you have to remember and type out all the types for all the columns in your csv file. I ended up doing the following. The idea here is to **store a single constant dictionary containing the type information for all the columns in all the data in a project.** Then write some helper functions to load data as follows. 

<pre><code class="language-python">
MASTER_TYPES: PandasTypes = {
    "fips": str,
    "naics": str,
    "weight": float,
    "year": "int16",
    "month": "int8",
    "qtr": "int8",
    ...
}

def read_cols(path: PathLike):
    return pd.read_csv(path, nrows=0).columns.tolist()

def load(path)
    cols = read_cols(path)
    dtypes = {c: MASTER_TYPES[c] for c in cols}
    return pd.read_csv(path, dtype=dtypes)
</code></pre>

The constraint here is that if you have two datasets with the same column names, they must have the same type, but this seems somewhat reasonable. In practice, I actually have some more helper functions that save the data in both feather-format and csv because feather loads much quicker. I also have another wrapper to refer to datasets by name instead of by path.

## Style and readability
Pandas code can get hella confusing. You might start off by loading in a `DataFrame` then start performing merges, adding columns, call another function on the DataFrame. While this is happening, someone else reading the code is sure to get lost. I found this happening to myself countless of times when getting acquainted with the code base. I'd wonder *'What columns are in the DataFrame at this point?'* and have to either put in print statements in the code or try and execute the code step by step in a Jupyter Notebook.

To try an improve the readability and style of the code I tried two main things - **method chaining**, and **'type/column'** checking. 

### Method chaining
Pandas has a nice thing were most of its functions return `DataFrames`. Even functions that you might no expect to such as `rename` or `astype` will return the `DataFrame` rather than changing them in place. **Method chaining makes the code easier to read because it very clearly shows the transformations being done data in a linear fashion**. Here's an example of this style of code using the [Education Statistics Dataset](https://www.kaggle.com/theworldbank/education-statistics). 

<pre><code class="language-python">
import pandas as pd
import plotly.express as px

COUNTRIES = [...]
COLS = [...]
COL2SHORT = dict(zip(COLS, ["m", "f"]))

(pd.read_csv("EdStatsData.csv")
    .drop(["Unnamed: 69", "Country Code", "Indicator Code"], axis=1)
    .query(f"`Indicator Name` == {COLS}")
    .query(f"`Country Name` == {COUNTRIES}")
    .melt(
        id_vars=["Country Name", "Indicator Name"],
        var_name="year",
        value_name="Primary Enrollment Rate (%)",
    )
    .astype({"year": int})
    .query("1985 <= year <= 2014")
    .dropna()
    .assign(Sex=lambda x: x["Indicator Name"].map(COL2SHORT))
    .pipe(
        px.bar,
        x="Country Name",
        y="Primary Enrollment Rate (%)",
        color="Sex",
        barmode="group",
        animation_frame="year",
        template="plotly_white",
    )
)
</code></pre>

Of note here, are the functions [assign](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html), and [pipe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pipe.html), which make this style of code possible. The raw data looks like this 

<pre><code class="language-python">
       Country Name Country Code                                     Indicator Name  ... 2095  2100  Unnamed: 69
0        Arab World          ARB  Adjusted net enrolment rate, lower secondary, ...  ...  NaN   NaN          NaN
1        Arab World          ARB  Adjusted net enrolment rate, lower secondary, ...  ...  NaN   NaN          NaN
2        Arab World          ARB  Adjusted net enrolment rate, lower secondary, ...  ...  NaN   NaN          NaN
3        Arab World          ARB  Adjusted net enrolment rate, lower secondary, ...  ...  NaN   NaN          NaN
4        Arab World          ARB  Adjusted net enrolment rate, primary, both sex...  ...  NaN   NaN          NaN
...             ...          ...                                                ...  ...  ...   ...          ...
886925     Zimbabwe          ZWE  Youth illiterate population, 15-24 years, male...  ...  NaN   NaN          NaN
886926     Zimbabwe          ZWE  Youth literacy rate, population 15-24 years, b...  ...  NaN   NaN          NaN
886927     Zimbabwe          ZWE  Youth literacy rate, population 15-24 years, f...  ...  NaN   NaN          NaN
886928     Zimbabwe          ZWE  Youth literacy rate, population 15-24 years, g...  ...  NaN   NaN          NaN
886929     Zimbabwe          ZWE  Youth literacy rate, population 15-24 years, m...  ...  NaN   NaN          NaN

</code></pre>

and after the transformations in the code above, we get this animation.

<script> 
$(function(){
    $("#includedContent").load("/plots/education_plot.html"); 
});
</script> 

<div id="includedContent"></div>

Also kinda unrelated but `plotly` is like 251x better than `matplotlib` for data viz in python. This style of code can be a pain to debug, but completely possible with some helper functions.

#### Helper functions
This function `pd_print` was really helpful in seeing how each operation transformed the data.

<pre><code class="language-python">
identity = lambda x: x
def pd_print(df, f=identity):
    print(f(df))
    return df
</code></pre>

Essentially it takes in a DataFrame and optionally a function, and prints the result of the function applied on the DataFrame and then returns the DataFrame unedited. Because it essentially does nothing to the DataFrame, we can litter our method chained calls with 

<pre><code class="language-python">
.pipe(pd_print)
</code></pre>

to print intermediate steps in our processing. We can use the f argument to just print the types if we want:

<pre><code class="language-python">
.pipe(pd_print, lambda x: x.dtypes)
</code></pre>

Here's another useful helper:

<pre><code class="language-python">
def pd_if(df, cond, f)):
    f(df) if cond else df
</code></pre>

This applies `f : pd.DataFrame -> pd.DataFrame` if cond is true and does nothing otherwise. Usage:
<pre><code class="language-python">
.pipe(pd_if, should_i_do_something, something)
</code></pre>

Where `should_i_do_something` is a boolean and `something` is a function to be called on the calling DataFrame when `should_i_do_something` is `True`.

You can extend both of these examples to pass along `*args` and/or `**kwargs` to f, which is supported by pipe :).

### Groupby
`groupby` is probably the easiest way in pandas to decompose a problem, and can make the development process much quicker. When you use `groupby(...).apply(...)` you only have to focus on writing/debugging the function for one of your groups. Here's what i do sometimes to get easy access to a single group.

<pre><code class="language-python">
df.groupby(...).apply(func)

def func(group):
    save_pkl(group, 'debug.pkl') # This isn't builtin but you can imaging what it does.
    exit()
</code></pre>

Then just load in `debug.pkl`, and check the correctness of `func` there.

Groupby apply is super general. Aside from outputting a new DataFrame, you can also use it to do things such as plot different groups or even split and save different groups to different files.

### Type and column checking
The idea here is to enforce the existence of certain columns and their types in the inputs and outputs of functions. To do this, I implemented decorator generator that works on functions with DataFrames as arguments or outputs. It can be used like this

<pre><code class="language-python">
@pd_type_check([TYPES1, TYPES2], TYPES3)
def some_function_on_dfs(df1, df2):
    ...
</code></pre>

Then, whenever this function is called, the decorated function first checks that the columns and types of the arguments are exactly those specified by `TYPES1` and `TYPES2`, then runs the function, and checks that the output has the exact types and columns specified by `TYPES3`. If any of the checks fail, it will raise and error. This **removes any ambiguity as to which columns and types the DataFrame has before/after a function.**

Currently, I'm extending this to work with arguments that can have a variable number of columns, or have columns specified by another argument to the function and will release this eventually.

## Conclusion

Pandas is a great library and can save so much time when working with data. Though it's also easy for pandas code to get insanely messy. If you're working with larger datasets, pandas also won't be that great (as it needs everything to be in memory). Dask is a library that implements most of the pandas api but works in a distributed way so that not all of the data needs to be in memory at once, however I have found it to be kinda annoying to use.